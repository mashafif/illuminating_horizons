{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953d21d5-6632-48ca-906a-1f7c3d8f94db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batches: 10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Process the DataFrame in batches with thread pooling\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m processed_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_large_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_processing_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# processed_df now contains the results of applying sample_processing_function to each row in the DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 71\u001b[0m, in \u001b[0;36mprocess_large_dataframe\u001b[0;34m(df, processing_function, batch_size, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m save_checkpoint(total_batches)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Concatenate the processed batches into a single DataFrame\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# If the input was a GeoDataFrame, ensure the output is also a GeoDataFrame\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, gpd\u001b[38;5;241m.\u001b[39mGeoDataFrame):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/illuminating_horizons/lib/python3.10/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/illuminating_horizons/lib/python3.10/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/illuminating_horizons/lib/python3.10/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "CHECKPOINT_FILE = os.path.join(SCRIPT_DIR, 'checkpoint.json')\n",
    "\n",
    "def save_checkpoint(i):\n",
    "    checkpoint_data = {'i': i}\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(checkpoint_data, f)\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "        return checkpoint_data['i']\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def process_batch(batch, processing_function, **kwargs):\n",
    "    processed_batch = batch.apply(lambda row: processing_function(row, **kwargs), axis=1)\n",
    "    return processed_batch\n",
    "\n",
    "def process_large_dataframe(df, processing_function, batch_size=1000, num_workers=4, **kwargs):\n",
    "    \"\"\"\n",
    "    Generalized function to process large DataFrames/GeoDataFrames in batches using thread pooling.\n",
    "\n",
    "    :param df: The DataFrame or GeoDataFrame to be processed.\n",
    "    :param processing_function: The function to apply to each row in the DataFrame.\n",
    "    :param batch_size: Number of rows to process in each batch.\n",
    "    :param num_workers: Number of worker threads to use.\n",
    "    :param kwargs: Additional keyword arguments to pass to the processing_function.\n",
    "    :return: Processed DataFrame or GeoDataFrame.\n",
    "    \"\"\"\n",
    "    start_index = load_checkpoint()\n",
    "\n",
    "    total_batches = len(df) // batch_size + (1 if len(df) % batch_size else 0)\n",
    "    print(f\"Total Batches: {total_batches}\")\n",
    "    \n",
    "    processed_dfs = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for i in range(start_index, total_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i + 1) * batch_size, len(df))\n",
    "            batch = df.iloc[start:end]\n",
    "            \n",
    "            futures.append(executor.submit(process_batch, batch, processing_function, **kwargs))\n",
    "            \n",
    "            save_checkpoint(i)\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                processed_batch = future.result()\n",
    "                processed_dfs.append(processed_batch)\n",
    "                print(f\"Processed batch {i + 1} / {total_batches}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during batch processing: {e}\")\n",
    "                save_checkpoint(i)\n",
    "                break\n",
    "\n",
    "    save_checkpoint(total_batches)\n",
    "\n",
    "    # Concatenate the processed batches into a single DataFrame\n",
    "    result_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "    # If the input was a GeoDataFrame, ensure the output is also a GeoDataFrame\n",
    "    if isinstance(df, gpd.GeoDataFrame):\n",
    "        result_df = gpd.GeoDataFrame(result_df, geometry='geometry', crs=df.crs)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# Example usage:\n",
    "# Define a sample processing function\n",
    "def sample_processing_function(row, additional_param):\n",
    "    # Replace this with your actual processing logic\n",
    "    row['processed_value'] = row['value'] * additional_param\n",
    "    return row\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'value': list(range(10000))}  # Replace with your actual data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Process the DataFrame in batches with thread pooling\n",
    "processed_df = process_large_dataframe(df, sample_processing_function, batch_size=1000, num_workers=4, additional_param=2)\n",
    "\n",
    "# processed_df now contains the results of applying sample_processing_function to each row in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8e5a22-cb14-4e08-a65f-59b3ced9d544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>processed_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>additional_paramadditional_paramadditional_par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value                                    processed_value\n",
       "0      1000  additional_paramadditional_paramadditional_par...\n",
       "1      1001  additional_paramadditional_paramadditional_par...\n",
       "2      1002  additional_paramadditional_paramadditional_par...\n",
       "3      1003  additional_paramadditional_paramadditional_par...\n",
       "4      1004  additional_paramadditional_paramadditional_par...\n",
       "...     ...                                                ...\n",
       "9995   9995  additional_paramadditional_paramadditional_par...\n",
       "9996   9996  additional_paramadditional_paramadditional_par...\n",
       "9997   9997  additional_paramadditional_paramadditional_par...\n",
       "9998   9998  additional_paramadditional_paramadditional_par...\n",
       "9999   9999  additional_paramadditional_paramadditional_par...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea9f17-cb25-48e1-b37e-3f9c90f839bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
